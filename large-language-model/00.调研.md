## 需求
* 开源大语言模型
* 硬件门槛不高
* 中文语言友好
* 支持免费商用
* 支持本地部署
* 支持内容审核([moderation api](https://platform.openai.com/docs/guides/moderation))
  
## 国外
* LLama-2-7b-chat-hf([huggingface](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)、[github](https://github.com/meta-llama/llama/tree/llama_v2))
  * [硬件要求](https://github.com/meta-llama/llama/issues/425)
    * RTX 3090 (24GB) 
      - llama-2-13b-chat.ggmlv3.q4_0.bin (offloaded 43/43 layers to GPU): 62.81 tokens per second
      - llama-2-13b-chat.ggmlv3.q8_0.bin (offloaded 43/43 layers to GPU): 36.39 tokens per second
    * 2x RTX 3080 (10GB)
      - llama-2-13b-chat.ggmlv3.q4_0.bin (offloaded 43/43 layers to GPU): 33.27 tokens per second
      - llama-2-13b-chat.ggmlv3.q8_0.bin (offloaded 43/43 layers to GPU): 28.27 tokens per second
    * RTX 3060 Ti 8GB
      - llama-2-13b-chat.ggmlv3.q4_0.bin (offloaded 38/43 layers to GPU): 11.06 tokens per second
      - llama-2-13b-chat.ggmlv3.q8_0.bin (offloaded 21/43 layers to GPU):  2.56 tokens per second

## 国内
* ChatGLM3([huggingface](https://huggingface.co/THUDM/chatglm3-6b)、[github](https://github.com/THUDM/ChatGLM3))
  * [硬件要求](https://github.com/THUDM/ChatGLM3)
    * RTX 3090 (24GB)
    * RTX 3080 (10GB)